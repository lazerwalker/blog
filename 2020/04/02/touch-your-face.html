<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />

  <title>
    How YOU Can Use Computer Vision to Avoid Touching Your Face!
  </title>
  <meta name="description" content="Emilia is a Toronto-based game designer, conceptual artist, and software engineer.
" />

  <link
    href="/atom.xml"
    rel="alternate"
    title="Em Lazer-Walker"
    type="application/atom+xml"
  />

  <link rel="stylesheet" href="/css/main.css">
  <link
    rel="canonical"
    href="http://blog.lazerwalker.com/2020/04/02/touch-your-face.html"
  />
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-STKNZZJ7SW"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-STKNZZJ7SW');
  </script>

  /opengraph/0bc6c41a3903d1c3408e3734d7d3ca0d2a94229f3719789fab731c22ede4f5ff.png
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Emilia Lazer-Walker</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">How YOU Can Use Computer Vision to Avoid Touching Your Face!</h1>
    <p class="post-meta">Apr 2, 2020</p>
  </header>

  <article class="post-content">
    <p>When COVID-19 was first starting to spread, well before people outside Wuhan were sheltering-in-place and social distancing, the sole advice being spread on social media was to not touch your face. This led me to a terrifying realization: I touch my face a lot. Like, <em>a lot</em>.</p>

<p>Of course, working in technology, I‚Äôm cursed to view every problem as one that can be solved with software. I ended up building a website that uses your webcam data and some machine learning to determine whether or not you‚Äôre touching your face. The instant a stray finger scratches your nose or repositions your hair, the app loudly honks at you!</p>

<center style="margin: 2em;">
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">If your fingers are loose, you&#39;ll get the goose!<br /><br />Built using <a href="https://twitter.com/Azure?ref_src=twsrc%5Etfw">@azure</a> Custom Vision Service. Tutorial + blog post coming soon! (üîàon!) <a href="https://t.co/cHKzTbzl0u">pic.twitter.com/cHKzTbzl0u</a></p>&mdash; emilia ‚ú® (@lazerwalker) <a href="https://twitter.com/lazerwalker/status/1235327787130646532?ref_src=twsrc%5Etfw">March 4, 2020</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<em>My original prototype</em>
</center>

<p>This is built using a tool called <a href="https://azure.microsoft.com/en-us/services/cognitive-services/custom-vision-service/?WT.mc.id=aiapril-medium-emwalker">Custom Vision</a>, part of Microsoft Azure Cognitive Services, that makes it easy to train your own deep learning computer vision model. This was my first time using Custom Vision, or any sort of method at all of training a custom machine-learning model to do CV. I was amazed at how easy it was to train a working model!</p>

<p>It turns out ‚Äútraining a machine learning model‚Äù isn‚Äôt some scary piece of dark arts that requires you to have a PhD in machine learning. As I walk you through my journey, you‚Äôll learn as I did that it can literally be as easy as uploading some images to a website!</p>

<h2 id="first-try-face-detection">First Try: Face Detection!</h2>
<p>At first, building a custom model didn‚Äôt even cross my mind. I started by playing around with a few other high-level APIs within Azure Cognitive Services, such as the Facial Recognition service. It basically exists to answer questions like ‚Äúare there faces in ths image}‚Äù and ‚Äúif so, where are they in the image?‚Äù, but it happens to also return data about whether or not those faces are occluded.</p>

<center style="margin: 2em">
    <a href="/images/face-app/face-1.png"><img src="/images/face-app/face-1.png" alt="The Azure Face Detection API" style="max-height: 400px" /></a>
</center>

<p>It works really well, but the data is incomplete! You can see in that API response above that it includes some occlusion data about whether a face‚Äôs eyes, nose, or mouth were covered, but nothing as broad as ‚Äúthere is something covering any part of the face‚Äù. It seems likely the service has that data somewhere, but it sadly isn‚Äôt exposed to us as users of that API.</p>

<h2 id="second-try-a-custom-visionmodel">Second Try: A Custom Vision¬†Model!</h2>
<p>After speaking with a coworker, I was pointed in the direction of <a href="https://azure.microsoft.com/en-us/services/cognitive-services/custom-vision-service/#features?WT.mc.id=aiapril-medium-emwalker">Custom Vision</a>. It offers an easy drag-and-drop way to train custom vision models: just upload some images, tag them, and let the machine learning model figure out the rest.</p>

<p>Specifically, it uses a powerful technique called transfer learning. Instead of training an image-detection neural network from scratch, it‚Äôs starting with a very large pretrained model that has awareness of all sorts of images. When I upload images of people‚Äôs faces, and people touching their faces, that gets used to subtrain the existing model. It‚Äôs like the neural network is starting with some base knowledge of what a face is and what hands are, I‚Äôm just helping show it what I‚Äôm particularly interested in.</p>

<p>It goes without saying this is all a massive simplification of what‚Äôs happening under the hood. I‚Äôm not a machine learning expert; the good news is this stuff is easy enough to use that I don‚Äôt need to be!</p>

<p>What‚Äôs great about this approach is it requires shockingly little data. I wrote a small script to fetch a few dozen images each from Bing Image Search for the terms ‚Äúface‚Äù and ‚Äútouching face‚Äù.</p>

<center style="margin: 2em">
    <a href="/images/face-app/face-2.png"><img src="/images/face-app/face-2.png" alt="Stock images for 'face' and 'touching face'" style="max-height: 300px" /></a>
    <a href="/images/face-app/face-3.png"><img src="/images/face-app/face-3.png" alt="More stock images for 'face' and 'touching face'" style="max-height: 300px" /></a>
</center>

<p>From there, I just created a new project in the <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/get-started-build-detector?WT.mc.id=aiapril-medium-emwalker">Custom Vision dashboard</a>, selected a model (‚ÄúGeneral‚Äù made sense!), uploaded my images, and tagged each of them (so each image was either ‚Äútouching-face‚Äù or ‚Äúnot-touching-face‚Äù).</p>

<p>With the dataset in place, clicking ‚ÄúTrain‚Äù took maybe 10 seconds to generate a functioning model, and I was given an API endpoint and secret key that would let me send image data and get back a response.</p>

<center style="margin: 2em">
    <a href="/images/face-app/face-4.png"><img src="/images/face-app/face-4.png" alt="A quick test of my model, where I'm touching my face and the probability of the 'touching face' tag is 99.99%" style="max-height: 400px" /></a>
</center>

<p>Shockingly, this worked really well! I built a quick webapp as a test, and while it got confused if there were no people visible at all and sometimes wouldn‚Äôt trigger if I was touching the very corner of my face, it was completely usable! I was blown away.</p>

<h2 id="a-catch-rate-limiting">A Catch: Rate Limiting?</h2>
<p>It‚Äôs great that this worked so well! However, while it worked really well as a tool for me to use for myself, it sadly wasn‚Äôt something I could release to a wider audience.</p>

<p>The way my original demo was written, I had a timer running 10 times a second to snapshot the current video feed and send that image up to the server. After some informal testing, this was a pretty good interval; if I ran it less frequently, it felt laggy and sluggish.</p>

<p>However, Custom Vision currently only lets you make at most 10 API calls per second for any given prediction model. Given I was already making 10 API calls per second, that meant this approach was impossible to scale beyond a single user!</p>

<h2 id="third-try-a-more-compact-mlmodel">Third Try: A More Compact ML¬†Model!</h2>
<p>Reducing the number of API calls to Custom Vision wasn‚Äôt really doable. Even if I changed the app to only send an image to the server once per second‚Ää-‚Ääwhich would make it basically unusable!‚Ää-‚Ääthat meant I could have a grand total of ten simultaneous users. A fundamentally different approach was needed.</p>

<p>I noticed that Custom Vision lets you export your model for use in a number of other environments. One of these is <a href="https://www.tensorflow.org/js">TensorFlow.js</a>, which runs directly in the web browser. If I could do that, I wouldn‚Äôt need to worry about API limits at all!</p>

<p>The bad news is that I needed to train a new model. The reason the previous model was so effective with so few sample images is because the corpus of images it had been pre-trained on was huge. As a result, that model was very large and computationally expensive to use. While this is fine when it‚Äôs running on Azure servers in the cloud, that makes it difficult to run in user‚Äôs browsers!</p>

<center style="margin: 2em">
    <a href="/images/face-app/face-5a.png"><img src="/images/face-app/face-5a.png" alt="Available domains for classification" style="max-height: 400px" /></a>
    <a href="/images/face-app/face-5.png"><img src="/images/face-app/face-5.png" alt="Available domains for object recognition" style="max-height: 400px" /></a><br />
    <em>Available domains for classification (left) and object recognition (right)</em>
</center>

<p>When creating a new model, Custom Vision gives you a handful of different pre-trained models you can base yours off of. Previously, I‚Äôd just used the ‚ÄúGeneral‚Äù domain. However, selecting any of the pre-trained models labeled ‚Äúcompact‚Äù means you‚Äôre starting with a smaller domain where your final model will be suitable for export to environments ranging from browser JavaScript to native iOS and Android to GPU-powered IoT devices.</p>

<p>The natural next step was to create a new model with a compact domain and re-train it Sadly, the results for just those 30 images were nowhere near as good as the original model.</p>

<p>This makes sense if you think about it: a smaller domain for our transfer learning means it‚Äôs going to have less initial context, which means it needs more training data to get a feel for what we‚Äôre asking of it.
I tried scraping a lot more generic images from Bing Image Search, but upping the number of images didn‚Äôt make a difference. I needed categorically better data.</p>

<p>I took a bunch of photos of myself using my webcam. Even with a very small dataset, this already seemed more promising (at least, testing it myself) than the stock image approach, but I needed a lot more data.</p>

<p>So I built a tool! Based on a coworker‚Äôs <a href="https://github.com/sethjuarez/vision">Custom Vision workshop project</a>, I made a website that walks people through taking photos with their webcam, tagging them correctly, and uploading them straight to the Custom Vision portal.</p>

<center style="margin: 2em">
    <a href="/images/face-app/face-6.png"><img src="/images/face-app/face-6.png" alt="A grid of photos of my face, tagged as 'touching-face' or 'not-touching' face, with a 'submit training data' web form button" style="max-height: 400px" /></a>
</center>

<p>You can play with this tool <a href="https://facetoucherstorage.z5.web.core.windows.net/">live online</a>, and even submit some data if you want! If you‚Äôre interested how it works, I‚Äôd recommend checking out my colleague‚Äôs <a href="http://github.com/sethjuarez/vision">workshop</a>, but in broad strokes this quickly gave me a web-based tool I could send to friends and colleagues to help me generate a lot of data quickly.</p>

<p>After gathering 200‚Äì300 images each, results started looking better! The resulting model still wasn‚Äôt up to the quality of the original one, but it was within the realm of what I could control with some simple heuristics in code.</p>

<p>In other words: the default way you‚Äôd build something like this is to say ‚Äúif the probability of the ‚Äòtouching-face‚Äô tag is over a certain number, the user is touching their face!‚Äù. Instead, I‚Äôm being ever-so-slightly more nuanced, and looking at the relationship between the probabilities of both the ‚Äòtouching-face‚Äô and ‚Äònot-touching-face‚Äô tags to figure out whether the user has touched their face or not.</p>

<h2 id="building-it-out-into-an-actualwebapp">Building it out into an actual¬†webapp!</h2>
<p>Now that I had something that actually worked well enough‚Ää-‚Ääif not perfectly‚Ää-‚Ääentirely within the browser, it was time to flesh out the actual experience of using it!</p>

<p>You can <a href="https://lazerwalker.com/dont-touch-your-face">try the experience</a> yourself if you‚Äôd like! I felt it was important to give users the options of different sounds, and also add in features like letting them trigger system alerts that would foreground the page if they wanted to keep the app open in a background window.</p>

<center style="margin: 2em">
    <a href="/images/face-app/face-7.png"><img src="/images/face-app/face-7.png" alt="A screenshot of the web app while I am not touching my face in the webcam view. The website is a neutral color, and tells me it has been 31 seconds since I last touched my face" style="max-height: 400px; max-width: 48%" /></a>
    <a href="/images/face-app/face-8.png"><img src="/images/face-app/face-8.png" alt="A screenshot of the web app where I am touching my face in the webcam view. The page background is red, and big all-caps text tells me I am touching my face." style="max-height: 400px; max-width: 48%" /></a><br />
    <em>The live webapp as it exists today</em>
</center>

<p>I‚Äôm perhaps happiest with the ‚Äòhigh score‚Äô tracker at the bottom. I considered adding in Twitter authentication so that you could see a live leaderboard of all your friends via <a href="https://playfab.com/?WT.mc.id=aiapril-medium-emwalker">PlayFab</a>, but that seemed like a lot of complexity. Giving you a personal best to beat seemed like a much simpler way to give people some sort of extrinsic motivation to do better.</p>

<p>Another design decision I‚Äôm particularly proud of was including a link to my training data page. I‚Äôve been overwhelmed at how many submissions I‚Äôve received from strangers. I now have more than ten times as much training data, which means when I have the time to experiment I should be able to make it work much better!</p>

<p>I‚Äôm still figuring out what other cool things I can do with my trained model‚Ää-‚ÄäI‚Äôve exported it to Apple CoreML and am experimenting with building a native macOS app that lives in your menubar, and I have a colleague who‚Äôs been getting it running on GPU-accelerated IoT hardware.</p>

<p>But above all, I‚Äôm excited that I‚Äôve gained a new tool in my toolbox for building things. I literally would not have imagined before building this just how easy it is to build your own machine learning model to do object recognition or image classification. Now that I (and now you!) know that awesome AI/ML tools like the Custom Vision service exist, I‚Äôm excited to go out and build more weird things that are aware of objects in the world!</p>

<p>If you liked this post, be sure to check out <a href="https://aka.ms/aiapril">Azure AI April</a>, where new articles like this will be posted every day throughout April!</p>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Emilia Lazer-Walker</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li>Emilia Lazer-Walker</li>
          <li><a href="mailto:blog@lazerwalker.com">blog@lazerwalker.com</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/lazerwalker">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>

              <span class="username">lazerwalker</span>
            </a>
          </li>
          

          
          <li>
            <a href="https://twitter.com/lazerwalker">
              <span class="icon  icon--twitter">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                  c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
                </svg>
              </span>

              <span class="username">lazerwalker</span>
            </a>
          </li>
          
        </ul>
      </div>

      <div class="footer-col  footer-col-3">
        <p class="text">Emilia is a Toronto-based game designer, conceptual artist, and software engineer.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
